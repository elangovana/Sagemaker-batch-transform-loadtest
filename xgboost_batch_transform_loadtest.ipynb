{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capacity test XGBoost Batch transform\n",
    "This sample test 10,000,000 millon rows with 70 columns each using XGBoost Builtin algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify jobname\n",
    "sagemaker_training_job_name = \"xgboost-2019-01-21-00-33-32-349\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_size = 69 # This depends on your model columns\n",
    "rows_size = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket=sess.default_bucket() \n",
    "\n",
    "prefix = \"xgboost_capacity_test_2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_batch_transform_prefix = \"s3://{}/{}/{}/batch_transform/\".format(bucket, prefix, sagemaker_training_job_name)\n",
    "s3_batch_transform_input_prefix = \"{}input/\".format(s3_batch_transform_prefix)\n",
    "s3_batch_transform_output_prefix = \"{}output/\".format(s3_batch_transform_prefix)\n",
    "s3_batch_transform_output_merged_prefix = \"{}mergedoutput/\".format(s3_batch_transform_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = sagemaker_training_job_name\n",
    "model_name = job_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "hosting_image = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a one time operation per training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "sage = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "\n",
    "info = sage.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "\n",
    "primary_container = {\n",
    "    'Image': hosting_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sage.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy large data set and upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = \"tmp_xgboost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.19 s, sys: 2.01 s, total: 8.2 s\n",
      "Wall time: 8.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(rows_size, columns_size)))\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 69)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write one large dataframe into  smaller csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "input_dir= \"{}/{}\".format(tempdir, \"input_parts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $input_dir\n",
    "!mkdir -p $input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Slow op... 8 mins on m4.xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 23s, sys: 45 s, total: 10min 8s\n",
      "Wall time: 8min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "chunk_size_num_records = 10000\n",
    "\n",
    "\n",
    "for i in range(0, df.shape[0], chunk_size_num_records):\n",
    "    df.iloc[i:i + chunk_size_num_records,].to_csv('{}/input_csv_part_{}.csv'.format(input_dir, i), index=False, header = False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.9G\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 2.0M Mar 25 09:50 input_csv_part_0.csv\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 2.0M Mar 25 09:51 input_csv_part_1000000.csv\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $input_dir | head -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has create smaller files of 2 mb each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 226 ms, sys: 96.8 ms, total: 322 ms\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!aws s3 sync $input_dir $s3_batch_transform_input_prefix --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift: Unload csv from redshift to S3 into smaller files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See example here https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD_command_examples.html. \n",
    "- Restrict max file size to say, 2 MB\n",
    "\n",
    "```sql\n",
    "unload ('select * from venue')\n",
    "to 's3://mybucket/unload/' \n",
    "iam_role 'arn:aws:iam::0123456789012:role/MyRedshiftRole'\n",
    "maxfilesize 2 mb;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 utilities\n",
    "S3 utlities to upload and download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def uploadfile(localpath, s3path):\n",
    "        \"\"\"\n",
    "Uploads a file to s3\n",
    "        :param localpath: The local path\n",
    "        :param s3path: The s3 path in format s3://mybucket/mydir/mysample.txt\n",
    "        \"\"\"\n",
    "\n",
    "        bucket, key = get_bucketname_key(s3path)\n",
    "\n",
    "        if key.endswith(\"/\"):\n",
    "            key = \"{}{}\".format(key, os.path.basename(localpath))\n",
    "        \n",
    "        s3 = boto3.client('s3')\n",
    "        \n",
    "        s3.upload_file(localpath, bucket, key)\n",
    "\n",
    "def get_bucketname_key(uripath):\n",
    "    assert uripath.startswith(\"s3://\")\n",
    "\n",
    "    path_without_scheme = uripath[5:]\n",
    "    bucket_end_index = path_without_scheme.find(\"/\")\n",
    "\n",
    "    bucket_name = path_without_scheme\n",
    "    key = \"/\"\n",
    "    if bucket_end_index > -1:\n",
    "        bucket_name = path_without_scheme[0:bucket_end_index]\n",
    "        key = path_without_scheme[bucket_end_index + 1:]\n",
    "\n",
    "    return bucket_name, key\n",
    "\n",
    "\n",
    "def download_file(s3path, local_dir):\n",
    "    bucket, key = get_bucketname_key(s3path)\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    local_file = os.path.join(local_dir, s3path.split(\"/\")[-1])\n",
    "    \n",
    "\n",
    "    s3.download_file(bucket, key, local_file)\n",
    "    \n",
    "\n",
    "def list_files(s3path_prefix):\n",
    "    assert s3path_prefix.startswith(\"s3://\")\n",
    "    assert s3path_prefix.endswith(\"/\")\n",
    "    \n",
    "    bucket, key = get_bucketname_key(s3path_prefix)\n",
    "    \n",
    "   \n",
    "   \n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    bucket = s3.Bucket(name=bucket)\n",
    "\n",
    "    return ( (o.bucket_name, o.key) for o in bucket.objects.filter(Prefix=key))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "\n",
    "def upload_files(local_dir, s3_prefix, num_threads=100):    \n",
    "    input_tuples = ( (f,  s3_prefix) for f in glob.glob(\"{}/*\".format(local_dir)))\n",
    "    \n",
    "    with ThreadPool(num_threads) as pool:\n",
    "        pool.starmap(uploadfile, input_tuples)\n",
    "    \n",
    "\n",
    "\n",
    "def download_files(s3_prefix, local_dir, num_threads=100):    \n",
    "    input_tuples = ( (\"s3://{}/{}\".format(s3_bucket,s3_key),  local_dir) for s3_bucket, s3_key in list_files(s3_prefix))\n",
    "    \n",
    "    with ThreadPool(num_threads) as pool:\n",
    "        results = pool.starmap(download_file, input_tuples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 s, sys: 14.7 s, total: 48.5 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "upload_files( input_dir, s3_batch_transform_input_prefix, num_threads=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starts here : Run batch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27 µs, sys: 2 µs, total: 29 µs\n",
      "Wall time: 31 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import os\n",
    "from time import strftime , gmtime\n",
    "\n",
    "\n",
    "fmttime= strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "input_location =  s3_batch_transform_input_prefix\n",
    "output_location = \"{}{}/\".format(s3_batch_transform_output_prefix, fmttime)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to s3://sagemaker-us-east-2-324346001917/xgboost_capacity_test_2/xgboost-2019-01-21-00-33-32-349/batch_transform/output/2019-03-25-10-00-19/\n"
     ]
    }
   ],
   "source": [
    "print(\"Results will be saved to {}\".format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run transform on a single ml.c4.xlarge instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: Batch-Transform-2019-03-25-10-01-46-604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................!\n",
      "CPU times: user 281 ms, sys: 32.1 ms, total: 313 ms\n",
      "Wall time: 5min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Initialize the transformer object\n",
    "transformer =sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name='Batch-Transform',\n",
    "    model_name=model_name,\n",
    "    max_payload = 5,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    output_path=output_location\n",
    "    )\n",
    "# To start a transform job:\n",
    "transformer.transform(input_location, content_type='text/csv', split_type='Line')\n",
    "# Then wait until transform job is completed\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy batch results locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_output_dir= \"{}/batch_out\".format(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf local_output_dir\n",
    "!mkdir -p $local_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.8 s, sys: 2.58 s, total: 28.4 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "download_files( output_location , local_output_dir, num_threads=130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.958344399929\r\n",
      "0.958344399929\r\n",
      "0.950590014458\r\n",
      "0.954473614693\r\n",
      "0.950590014458\r\n",
      "0.925055503845\r\n",
      "0.950590014458\r\n",
      "0.958697319031\r\n",
      "0.966336548328\r\n",
      "0.950590014458\r\n"
     ]
    }
   ],
   "source": [
    "!head $local_output_dir/input_csv_part_9960000.csv.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the results from batch transform.\n",
    "\n",
    "The batch tranform results only contain the results and not the corresponding input features. You need to merge the resulst (confidence scores) with the id of the input to later use it to update redshift or any other system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fetch validation result \n",
    "import glob\n",
    "import os\n",
    "import pandas\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import pathlib\n",
    "\n",
    "\n",
    "\n",
    "# def merge_input_output(input_id_file, batch_results_file, output_dir):\n",
    "#     \"\"\"\n",
    "#     Merge the id of the input and the results (confidence score), this is slow..\n",
    "#     \"\"\"\n",
    "#     df_i = pd.read_csv(input_id_file, header=None)\n",
    "#     df_o = pd.read_csv(batch_results_file, header= None)\n",
    "    \n",
    "#     df_i[\"confidence_score\"] = df_o.iloc[:, 0]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     df_i.to_csv(results_path)\n",
    "    \n",
    "#     return results_path\n",
    "  \n",
    "\n",
    "def merge_input_output_raw(input_id_file, batch_results_file, output_dir):\n",
    "    \"\"\"\n",
    "    Merge the id of the input and the results (confidence score)\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    # input id files\n",
    "    with open(input_id_file, 'r') as input_csv:\n",
    "        i_reader = csv.reader(input_csv, delimiter=',')\n",
    "        \n",
    "        #Confidence scores\n",
    "        with open(batch_results_file,  'r') as scores_csv:\n",
    "            s_reader = csv.reader(scores_csv, delimiter=',')        \n",
    "            \n",
    "            # Output file\n",
    "            results_path = os.path.join(output_dir, os.path.basename(input_id_file))\n",
    "            with open(results_path, \"w\") as o:\n",
    "                writer = csv.writer(o, delimiter=',')\n",
    "                \n",
    "                # Merge input, confidence and write to file\n",
    "                for i, s in zip(i_reader, s_reader):\n",
    "                    i.extend(s)\n",
    "                    writer.writerow(i)\n",
    "  \n",
    "    \n",
    "    return results_path\n",
    "  \n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "def get_input_confidence_file_tuples(input_file_dir, local_batch_output_dir):\n",
    "    for f in glob.glob(\"{}/*.out\".format(local_batch_output_dir)):\n",
    "        i_file_name =  os.path.basename(f).replace(\".out\", \"\")\n",
    "        input_file_path= os.path.join(input_file_dir, i_file_name)\n",
    "        yield (input_file_path, f)\n",
    "    \n",
    "\n",
    "def merge_files(num_threads , input_file_dir, local_batch_dir, out_dir):\n",
    "    #Create output dir\n",
    "    pathlib.Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with ThreadPool(num_threads) as pool:\n",
    "        merge_tuples = ((i,o, out_dir) for i,o in get_input_confidence_file_tuples(input_file_dir, local_batch_dir))\n",
    "        results = pool.starmap(merge_input_output_raw, merge_tuples)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 minute to complete depending on the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 4.47 s, total: 1min 20s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_dir = os.path.join(tempdir, \"results\")\n",
    "\n",
    "merge_files(num_threads = 30, input_file_dir = input_dir, local_batch_dir= local_output_dir, out_dir=results_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.5 s, sys: 15 s, total: 48.5 s\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "upload_files(results_dir, s3_batch_transform_output_merged_prefix, num_threads=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift copy command to load the results into s3\n",
    "\n",
    "This command will upload multiple files from s3://mybucket/prefix to table temp_customer\n",
    "For more detail see https://docs.aws.amazon.com/redshift/latest/dg/t_loading-tables-from-s3.html \n",
    "\n",
    "```sql\n",
    "copy temp_customer \n",
    "from 's3://mybucket/prefix' \n",
    "iam_role 'arn:aws:iam::0123456789012:role/MyRedshiftRole';\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
